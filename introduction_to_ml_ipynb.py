# -*- coding: utf-8 -*-
"""Introduction to ML. ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DbGlHZtTZgN7iwIAuKFEBO44WpsMDrr2

**Eric Otieno Onyango**

## Background Information
HR analytics is revolutionising the way human resources departments operate, leading
to higher efficiency and better results overall. Human resources have been using
analytics for years. However, the collection, processing, and analysis of data have been
largely manual, and given the nature of human resources dynamics and HR KPIs, the
approach has been constraining HR. Therefore, it is surprising that HR departments
woke up to the utility of machine learning so late in the game.
"""

#In this cell we load the important packages 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# We read the data

df = pd.read_csv("https://bit.ly/2ODZvLCHRDataset")

df.sample(10) #previewing the random 10 records

"""## Exploratory Data Analysis

**A General Looking at the Data**
"""

df.info()

"""**We can see, we have some columns with missing data**"""

#Details on the Categorical data 
df.describe(include=object).T

#Details on the Numerical data 
numerical= df.select_dtypes('number').columns
df[numerical].describe().T

#custom function for getting and displaying Missing values

def missing (df):
    missing_number = df.isnull().sum().sort_values(ascending=False)
    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])
    return missing_values

#checking for missing data using the custom method --missing
missing(df)

# Another alternative for checking the missing data would be
#df.isnull().sum()

"""**Evidently, previous_year_rating and education have 4124 and 2409 values missing**"""



"""Distribution of the target variable is one of the most important things in a classification problem. So let's a close look at how its values are distributed."""

df['is_promoted'].value_counts()

#we can plot a pairplt
import seaborn as sns

sns.pairplot(df, hue="is_promoted");

"""# **Data Cleaning**"""

#id will be unnecessary for predictions, we drop it
df.drop('employee_id', axis=1, inplace=True)

# We can fill in the missing values for  previous_year_rating and education using their median
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(missing_values=np.nan, strategy="median")
df['previous_year_rating'] = imputer.fit_transform(df['previous_year_rating'].values.reshape(-1,1))[:,0]

missing(df)

#Imputing Education Data (Categorical Data) with the most frequent value 
imputer = SimpleImputer(missing_values=np.nan, strategy="most_frequent")
df['education'] = imputer.fit_transform(df['education'].values.reshape(-1,1))[:,0]

missing(df)

df.sample(10)

"""**Convert categorical variable into dummy/indicator variables.**"""

df = pd.get_dummies(df, columns=['department', 'region', 'gender', 'recruitment_channel', 'education'], drop_first=False)

df.info()

df.sample(5)

"""# **The Implementattion of Logistic Methods**

**### Train | Test Split and Scaling**
"""

X = df.drop(columns=["is_promoted"])
y = df.is_promoted

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""**Scaling**"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""### Modeling"""

from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression(class_weight="balanced").fit(X_train_scaled, y_train)
log_model

y_test_pred = log_model.predict(X_test_scaled)
y_pred_proba = log_model.predict_proba(X_test_scaled)

test_data = pd.concat([X_test, y_test], axis=1)
test_data["pred"] = y_test_pred
test_data["pred_proba"] = y_pred_proba[:,1]
test_data.sample(10)

"""# Model Performance on Classification Tasks"""

from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print(confusion_matrix(y_test,y_test_pred))
cm = confusion_matrix(y_test, y_test_pred, labels=log_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=log_model.classes_)
disp.plot()
plt.show()
print(classification_report(y_test,y_test_pred))



"""# Conclusion 
* this can be treated as a classification issue
* in the begining Exploratory Data Analysis was done
* target (y), and other features (x) analyzed
* handled the missing values
* logisitic regression model was established
* at the end, obtained 0.77 (77 percent) accuracy. However does badly when it comes to the precision of predicting the the class 1.

* We can explore other models, like descion trees, support vector machine and compare the metrics

"""